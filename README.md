# Sistema RAG (Retrieval-Augmented Generation) com Chat CLI

Este projeto implementa um sistema completo de Retrieval-Augmented Generation (RAG) que permite fazer perguntas sobre documentos PDF usando um chat interativo no terminal. O sistema vetoriza documentos, armazena em um banco vetorial PostgreSQL e responde perguntas baseadas exclusivamente no conteÃºdo dos documentos.

## ğŸ“‹ Ãndice

- [Arquitetura do Projeto](#-arquitetura-do-projeto)
- [Tecnologias e DependÃªncias](#-tecnologias-e-dependÃªncias)
- [ConfiguraÃ§Ã£o do Ambiente](#-configuraÃ§Ã£o-do-ambiente)
- [ConfiguraÃ§Ã£o de APIs](#-configuraÃ§Ã£o-de-apis)
- [ConfiguraÃ§Ã£o do Docker](#-configuraÃ§Ã£o-do-docker)
- [VariÃ¡veis de Ambiente](#-variÃ¡veis-de-ambiente)
- [Como Usar](#-como-usar)
- [Recursos do Chat](#-recursos-do-chat)
- [ConfiguraÃ§Ãµes AvanÃ§adas](#-configuraÃ§Ãµes-avanÃ§adas)
- [ResoluÃ§Ã£o de Problemas](#-resoluÃ§Ã£o-de-problemas)

## ğŸ—ï¸ Arquitetura do Projeto

O sistema Ã© composto por trÃªs componentes principais:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IngestÃ£o      â”‚    â”‚  Banco Vetorial â”‚    â”‚      Chat       â”‚
â”‚   (ingest.py)   â”‚â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚â—„â”€â”€â”€â”‚   (chat.py)     â”‚
â”‚                 â”‚    â”‚   + pgvector    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Loader     â”‚    â”‚   Embeddings    â”‚    â”‚  Search Engine  â”‚
â”‚  Text Splitter  â”‚    â”‚   Vector Store  â”‚    â”‚  LLM Integrationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **IngestÃ£o**: PDFs sÃ£o carregados, divididos em chunks e vetorizados
2. **Armazenamento**: Embeddings sÃ£o salvos no PostgreSQL com pgvector
3. **Consulta**: Perguntas sÃ£o vetorizadas e buscam documentos similares
4. **Resposta**: LLM gera respostas baseadas apenas no contexto encontrado

## ğŸ› ï¸ Tecnologias e DependÃªncias

### Frameworks Principais
- **LangChain**: Framework para aplicaÃ§Ãµes com LLM
- **LangChain Community**: ExtensÃµes e integraÃ§Ãµes
- **LangChain Text Splitters**: DivisÃ£o de documentos

### Modelos de LLM e Embeddings
- **Google Gemini**: Embeddings e modelos de linguagem
- **OpenAI**: GPT e embeddings da OpenAI
- **HuggingFace**: Embeddings locais (Sentence Transformers)

### Banco de Dados e Vetorial
- **PostgreSQL**: Banco de dados principal
- **pgvector**: ExtensÃ£o para busca vetorial
- **psycopg**: Driver Python para PostgreSQL

### Processamento de Documentos
- **PyPDF**: Carregamento de arquivos PDF
- **Sentence Transformers**: Embeddings locais
- **torch**: Framework de deep learning

### Infraestrutura
- **Docker**: ContainerizaÃ§Ã£o do PostgreSQL
- **python-dotenv**: Gerenciamento de variÃ¡veis de ambiente

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1. PrÃ©-requisitos
- Python 3.8 ou superior
- Docker e Docker Compose
- Git

### 2. Clone do RepositÃ³rio
```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca
```

### 3. Ambiente Virtual
```bash
# Criar ambiente virtual
python -m venv venv

# Ativar ambiente virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

### 4. InstalaÃ§Ã£o de DependÃªncias
```bash
pip install -r requirements.txt
```

## ğŸ”‘ ConfiguraÃ§Ã£o de APIs

### OpenAI API Key

1. **Acesse o site da OpenAI:**
   - [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)

2. **FaÃ§a login ou crie uma conta:**
   - Se jÃ¡ possui conta, faÃ§a login
   - Caso contrÃ¡rio, crie uma nova conta

3. **Crie uma nova API Key:**
   - Clique em "Create new secret key"
   - DÃª um nome identificÃ¡vel para a chave
   - Clique em "Create secret key"

4. **Copie e armazene sua API Key:**
   - A chave serÃ¡ exibida apenas uma vez
   - Copie e adicione no arquivo `.env`

**Tutorial completo:** [Como Gerar uma API Key na OpenAI](https://hub.asimov.academy/tutorial/como-gerar-uma-api-key-na-openai/)

### Google Gemini API Key

1. **Acesse o Google AI Studio:**
   - [https://ai.google.dev/gemini-api/docs/api-key?hl=pt-BR](https://ai.google.dev/gemini-api/docs/api-key?hl=pt-BR)

2. **FaÃ§a login com sua conta Google:**
   - Use sua conta Google existente

3. **Crie uma nova API Key:**
   - Clique em "Create API Key" ou "Criar chave de API"
   - DÃª um nome identificÃ¡vel
   - A chave serÃ¡ gerada e exibida

4. **Copie e armazene sua API Key:**
   - Copie a chave e adicione no arquivo `.env`

**DocumentaÃ§Ã£o oficial:** [Como usar chaves da API Gemini](https://ai.google.dev/gemini-api/docs/api-key?hl=pt-BR)

## ğŸ³ ConfiguraÃ§Ã£o do Docker

### 1. Iniciar PostgreSQL com pgvector
```bash
# Subir o banco de dados
docker-compose up -d

# Verificar se estÃ¡ rodando
docker-compose ps
```

### 2. ConfiguraÃ§Ã£o do docker-compose.yml
O arquivo jÃ¡ estÃ¡ configurado com:
- PostgreSQL 15 com extensÃ£o pgvector
- Porta 5432 exposta
- Volume persistente para dados
- VariÃ¡veis de ambiente prÃ©-configuradas

### 3. Parar os serviÃ§os
```bash
# Parar serviÃ§os
docker-compose down

# Parar e remover volumes (ATENÃ‡ÃƒO: apaga dados)
docker-compose down -v
```

## ğŸ”§ VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com as seguintes configuraÃ§Ãµes:

### ConfiguraÃ§Ã£o BÃ¡sica
```env
# Banco de Dados PostgreSQL
DATABASE_URL=postgresql://langchain:langchain@localhost:5432/langchain
PG_VECTOR_COLLECTION_NAME=documents

# Arquivo PDF
PDF_PATH=document.pdf
```

### ConfiguraÃ§Ã£o de Embeddings
```env
# Provedor de Embeddings: local, gemini, openai
EMBEDDING_PROVIDER=local

# Para embeddings locais (HuggingFace)
LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Para Google Gemini
GOOGLE_API_KEY=sua_chave_api_google_aqui
GEMINI_EMBEDDING_MODEL=models/embedding-001

# Para OpenAI
OPENAI_API_KEY=sua_chave_api_openai_aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

### ConfiguraÃ§Ã£o de LLM
```env
# Provedor de LLM: gemini, openai
LLM_PROVIDER=gemini

# Modelos especÃ­ficos
GEMINI_MODEL=gemini-1.5-flash
OPENAI_MODEL=gpt-3.5-turbo
```

## ğŸš€ Como Usar

### 1. PreparaÃ§Ã£o do Ambiente
```bash
# 1. Ativar ambiente virtual
venv\Scripts\activate  # Windows
# ou
source venv/bin/activate  # Linux/Mac

# 2. Subir banco de dados
docker-compose up -d

# 3. Verificar se PDF existe
ls document.pdf
```

### 2. IngestÃ£o de Documentos
```bash
cd src
python ingest.py
```

**O que acontece na ingestÃ£o:**
- âœ… Carrega o PDF especificado em `PDF_PATH`
- âœ… Divide o documento em chunks de 1000 caracteres com overlap de 150
- âœ… Gera embeddings para cada chunk usando o provedor configurado
- âœ… Armazena no PostgreSQL com pgvector
- âœ… Cria Ã­ndices para busca eficiente

**SaÃ­da esperada:**
```
ğŸ  Usando Embeddings Locais (HuggingFace)
âœ… 45 documentos inseridos com sucesso no vector store!
```

### 3. Iniciar Chat Interativo
```bash
cd src
python chat.py
```

## ğŸ¯ Recursos do Chat

### Interface do Chat
```
ğŸ¤– CHAT RAG - Consulta Baseada em Documentos
================================================================================
Digite suas perguntas e eu responderei baseado no conteÃºdo dos documentos.
Comandos especiais:
  - 'sair' ou 'quit': encerra o chat
  - 'limpar' ou 'clear': limpa a tela
  - 'ajuda' ou 'help': mostra esta mensagem
--------------------------------------------------------------------------------
ğŸ”§ Inicializando sistema...
âœ… Sistema inicializado com sucesso!

Digite 'ajuda' para ver os comandos disponÃ­veis.

ğŸ‘¤ VocÃª: _
```

### Comandos Especiais
| Comando | Aliases | DescriÃ§Ã£o |
|---------|---------|-----------|
| `ajuda` | `help` | Mostra comandos disponÃ­veis e dicas |
| `sair` | `quit`, `exit` | Encerra o chat |
| `limpar` | `clear` | Limpa a tela do terminal |

### Funcionalidades TÃ©cnicas
- âœ… **Busca SemÃ¢ntica**: Vetoriza pergunta e busca top 10 resultados
- âœ… **Contexto Relevante**: Concatena documentos encontrados
- âœ… **Prompt Rigoroso**: Template que evita alucinaÃ§Ãµes
- âœ… **Respostas Controladas**: Baseadas exclusivamente no contexto
- âœ… **Tratamento de Erros**: Mensagens informativas para problemas
- âœ… **Interface AmigÃ¡vel**: Emojis e formataÃ§Ã£o clara


## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Provedores de Embeddings

#### 1. Embeddings Locais (Recomendado para Desenvolvimento)
```env
EMBEDDING_PROVIDER=local
LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

**Vantagens:**
- âœ… Gratuito e sem limite de uso
- âœ… Funciona offline
- âœ… Boa qualidade para documentos em portuguÃªs/inglÃªs
- âœ… RÃ¡pido apÃ³s o primeiro carregamento

**Desvantagens:**
- âŒ Primeiro uso demora (download do modelo ~90MB)
- âŒ Usa CPU (mais lento que GPU)
- âŒ Ocupa espaÃ§o em disco

#### 2. Google Gemini Embeddings
```env
EMBEDDING_PROVIDER=gemini
GOOGLE_API_KEY=sua_chave_aqui
GEMINI_EMBEDDING_MODEL=models/embedding-001
```

**Vantagens:**
- âœ… Alta qualidade e otimizado para multilingual
- âœ… RÃ¡pido (processamento na nuvem)
- âœ… Sem uso de recursos locais

**Desvantagens:**
- âŒ Requer chave de API
- âŒ Custo por uso (verificar pricing)
- âŒ Requer conexÃ£o com internet

#### 3. OpenAI Embeddings
```env
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=sua_chave_aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

**Vantagens:**
- âœ… Excelente qualidade
- âœ… Muito rÃ¡pido
- âœ… Otimizado para RAG

**Desvantagens:**
- âŒ Requer chave de API
- âŒ Custo por token
- âŒ Requer conexÃ£o com internet

### Modelos de Embeddings DisponÃ­veis

| Provedor | Modelo | DimensÃµes | Contexto | ObservaÃ§Ãµes |
|----------|--------|-----------|----------|-------------|
| Local | all-MiniLM-L6-v2 | 384 | 256 tokens | Recomendado para inÃ­cio |
| Local | all-mpnet-base-v2 | 768 | 384 tokens | Maior qualidade |
| Gemini | embedding-001 | 768 | 2048 tokens | Multilingual |
| OpenAI | text-embedding-3-small | 1536 | 8191 tokens | Custo/benefÃ­cio |
| OpenAI | text-embedding-3-large | 3072 | 8191 tokens | MÃ¡xima qualidade |

### ConfiguraÃ§Ãµes de Chunking

Para documentos muito grandes ou muito tÃ©cnicos, vocÃª pode ajustar as configuraÃ§Ãµes no `ingest.py`:

```python
splits = RecursiveCharacterTextSplitter(
    chunk_size=1500,        # Aumentar para documentos tÃ©cnicos
    chunk_overlap=200,      # Mais overlap para melhor contexto
    add_start_index=False
).split_documents(docs)
```

### ConfiguraÃ§Ãµes de Busca

Para ajustar a quantidade de documentos retornados, modifique em `search.py`:

```python
docs = vector_store.similarity_search(question, k=15)  # Mais contexto
```

## ğŸ› ï¸ ResoluÃ§Ã£o de Problemas

### Problema: Erro de ConexÃ£o com Banco
```
psycopg2.OperationalError: could not connect to server
```

**SoluÃ§Ã£o:**
```bash
# Verificar se Docker estÃ¡ rodando
docker --version
docker-compose ps

# Reiniciar serviÃ§os
docker-compose down
docker-compose up -d

# Verificar logs
docker-compose logs postgres
```

### Problema: MÃ³dulo nÃ£o encontrado
```
ModuleNotFoundError: No module named 'langchain_huggingface'
```

**SoluÃ§Ã£o:**
```bash
# Ativar ambiente virtual
venv\Scripts\activate

# Reinstalar dependÃªncias
pip install -r requirements.txt
```

### Problema: Erro de API Key
```
Error: Invalid API key provided
```

**SoluÃ§Ã£o:**
1. Verificar se `.env` existe e tem as chaves corretas
2. Verificar se chaves nÃ£o expiraram
3. Verificar cotas/billing das APIs

### Problema: PDF nÃ£o encontrado
```
FileNotFoundError: document.pdf not found
```

**SoluÃ§Ã£o:**
```bash
# Verificar caminho do PDF no .env
PDF_PATH=./document.pdf

# Ou usar caminho absoluto
PDF_PATH=/caminho/completo/para/document.pdf
```

### Problema: Embeddings muito lentos (Local)
**SoluÃ§Ã£o:**
```env
# Usar modelo menor
LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Ou mudar para embeddings na nuvem
EMBEDDING_PROVIDER=gemini
```

### Problema: Respostas de baixa qualidade
**SoluÃ§Ãµes:**
1. **Melhorar chunking**: Aumentar `chunk_size` e `chunk_overlap`
2. **Mais contexto**: Aumentar `k` na busca vetorial
3. **Melhor embedding**: Usar OpenAI ou modelo local maior
4. **Melhor LLM**: Usar GPT-4 ou Gemini Pro

### Logs e Debug

Para debug avanÃ§ado, adicione prints nos arquivos:

```python
# Em search.py, na funÃ§Ã£o search_documents
print(f"Pergunta: {question}")
print(f"Documentos encontrados: {len(docs)}")
for i, doc in enumerate(docs):
    print(f"Doc {i}: {doc.page_content[:100]}...")
```

---

## ğŸ“ Suporte

Para dÃºvidas ou problemas nÃ£o cobertos neste guia:

1. Verifique os logs do Docker: `docker-compose logs`
2. Ative debug nos scripts Python
3. Consulte a documentaÃ§Ã£o oficial do LangChain
4. Verifique status das APIs (OpenAI/Google)

---

**Nota de SeguranÃ§a:** ğŸ”’ Nunca compartilhe suas chaves de API publicamente. Mantenha o arquivo `.env` sempre no `.gitignore`.
